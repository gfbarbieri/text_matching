{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Text Matching Example\n",
    "\n",
    "This notebook demonstrates how to use the FuzzyPredictor for text matching using fuzzy matching approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "## FOR NOTEBOOKS ONLY: ADD THE PROJECT ROOT TO THE PYTHON PATH\n",
    "########################################################################\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(\n",
    "    0, os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from text_prediction.predictors.distance.fuzzy import FuzzyPredictor\n",
    "from text_prediction.predictors.distance import ALGORITHMS, METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "For this example, we'll use a sample dataset of product names and their variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iphone 13 pro max</td>\n",
       "      <td>iPhone 13 Pro Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iphone 13 promax</td>\n",
       "      <td>iPhone 13 Pro Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iphone 13 pro</td>\n",
       "      <td>iPhone 13 Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iphone 13</td>\n",
       "      <td>iPhone 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samsung galaxy s21</td>\n",
       "      <td>Samsung Galaxy S21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    X              y_true\n",
       "0   iphone 13 pro max   iPhone 13 Pro Max\n",
       "1    iphone 13 promax   iPhone 13 Pro Max\n",
       "2       iphone 13 pro       iPhone 13 Pro\n",
       "3           iphone 13           iPhone 13\n",
       "4  samsung galaxy s21  Samsung Galaxy S21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/product_descriptions.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Text Search\n",
    "\n",
    "In this example, we have a large set of text data and we simply want to seach for instances of a certain query. For example, in a list of reported survey responses, we want to look for instances of \"iPhone 13 Pro Max\" in our data.\n",
    "\n",
    "Create a basic FuzzyPredictor that will use the levenshtein algorithm and the distance method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = FuzzyPredictor(algorithm=\"levenshtein\", method=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To similulate this type of search, we will fit the predictor only on the X, which resemble the survey responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone 13 pro max',\n",
       " 'iphone 13 promax',\n",
       " 'iphone 13 pro',\n",
       " 'iphone 13',\n",
       " 'samsung galaxy s21',\n",
       " 'samsung galaxy s21 ultra',\n",
       " 'samsung s21',\n",
       " 'samsung s21 ultra',\n",
       " 'macbook pro m1',\n",
       " 'macbook pro m1 max',\n",
       " 'macbook m1',\n",
       " 'macbook m1 pro',\n",
       " 'airpods pro 2nd gen',\n",
       " 'airpods pro 2',\n",
       " 'airpods pro second generation',\n",
       " 'airpods 2nd gen',\n",
       " 'airpods 3rd gen',\n",
       " 'airpods 3',\n",
       " 'galaxy buds pro',\n",
       " 'samsung buds pro',\n",
       " 'galaxy buds 2',\n",
       " 'samsung buds 2',\n",
       " 'tide original detergent',\n",
       " 'tide detergent original',\n",
       " 'tide pods original',\n",
       " 'tide pods spring meadow',\n",
       " 'tide spring meadow detergent',\n",
       " 'bounty paper towels',\n",
       " 'bounty select a size',\n",
       " 'bounty select size',\n",
       " 'charmin ultra soft',\n",
       " 'charmin toilet paper ultra soft',\n",
       " 'charmin ultra strong',\n",
       " 'clorox original',\n",
       " 'clorox bleach',\n",
       " 'clorox disinfecting wipes',\n",
       " 'lysol disinfectant spray',\n",
       " 'lysol spray',\n",
       " 'lysol wipes',\n",
       " 'dawn dish soap original',\n",
       " 'dawn original',\n",
       " 'dawn ultra dish soap',\n",
       " 'cheerios original',\n",
       " 'plain cheerios',\n",
       " 'honey nut cheerios',\n",
       " 'frosted flakes',\n",
       " 'kelloggs frosted flakes',\n",
       " 'fruit loops',\n",
       " 'froot loops',\n",
       " 'kelloggs fruit loops',\n",
       " 'coca cola original',\n",
       " 'coke classic',\n",
       " 'diet coke',\n",
       " 'coke zero',\n",
       " 'coca cola zero',\n",
       " 'pepsi original',\n",
       " 'diet pepsi',\n",
       " 'pepsi zero sugar',\n",
       " 'doritos nacho cheese',\n",
       " 'doritos cool ranch',\n",
       " 'lays classic',\n",
       " 'lays original',\n",
       " 'lays bbq',\n",
       " 'cheetos original',\n",
       " 'cheetos crunchy',\n",
       " 'cheetos flamin hot',\n",
       " 'oreo original',\n",
       " 'oreo double stuf',\n",
       " 'oreo mint creme',\n",
       " 'chips ahoy original',\n",
       " 'chips ahoy chewy',\n",
       " 'nutella hazelnut spread',\n",
       " 'jif creamy peanut butter',\n",
       " 'jif crunchy',\n",
       " 'skippy creamy',\n",
       " 'skippy natural pb',\n",
       " 'kraft mac and cheese original',\n",
       " 'kraft mac n cheese',\n",
       " 'kraft dinner original',\n",
       " 'campbell chicken noodle soup',\n",
       " 'campbells tomato soup',\n",
       " 'progresso chicken noodle',\n",
       " 'heinz ketchup',\n",
       " 'heinz tomato ketchup',\n",
       " \"french's yellow mustard\",\n",
       " 'frenchs mustard',\n",
       " 'sriracha hot sauce',\n",
       " 'sriracha sauce',\n",
       " 'tabasco original',\n",
       " 'tabasco sauce',\n",
       " 'ben and jerrys chunky monkey',\n",
       " 'ben & jerrys phish food',\n",
       " 'haagen dazs vanilla',\n",
       " 'haagen dazs chocolate',\n",
       " 'breyers natural vanilla',\n",
       " 'quilted northern ultra plush',\n",
       " 'quilted northern toilet paper',\n",
       " 'angel soft toilet paper',\n",
       " 'angel soft bath tissue',\n",
       " 'scott toilet paper',\n",
       " 'brawny paper towels',\n",
       " 'sparkle paper towels',\n",
       " 'viva paper towels',\n",
       " 'cascade complete pods',\n",
       " 'cascade dishwasher pods',\n",
       " 'finish quantum pods',\n",
       " 'finish dishwasher tabs',\n",
       " 'swiffer wet jet',\n",
       " 'swiffer wetjet refill',\n",
       " 'swiffer duster',\n",
       " 'febreze air fresh linen',\n",
       " 'febreze air effects',\n",
       " 'glade clean linen spray',\n",
       " 'glade plugins refill',\n",
       " 'air wick essential oils refill',\n",
       " 'air wick plug in',\n",
       " 'duracell aa batteries',\n",
       " 'duracell aaa',\n",
       " 'energizer aa',\n",
       " 'energizer aaa batteries',\n",
       " 'glad tall kitchen bags',\n",
       " 'glad trash bags',\n",
       " 'hefty ultra strong',\n",
       " 'ziploc freezer gallon',\n",
       " 'ziploc storage quart',\n",
       " 'reynolds wrap aluminum foil',\n",
       " 'reynolds heavy duty foil',\n",
       " 'glad press n seal',\n",
       " 'saran premium wrap',\n",
       " 'dixie paper plates',\n",
       " 'solo red cups',\n",
       " 'red solo cups',\n",
       " 'bounty napkins',\n",
       " 'vanity fair napkins']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_transform(df['X'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will search our survey responses for the product \"iPhone 13 Pro Max\". We can see that there about 4 likely instances, but only two of them contain the product name. In a production framework, you would do more to clean up the text data before you search.\n",
    "\n",
    "What's happening is that the predictor is performing a fuzzy match between the query and the product descriptions by calculating the Levenshtein distance between the two strings. The results are sorted by the distance ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('iphone 13 pro max', np.uint32(0)),\n",
       "  ('iphone 13 promax', np.uint32(1)),\n",
       "  ('iphone 13 pro', np.uint32(4)),\n",
       "  ('iphone 13', np.uint32(8)),\n",
       "  ('airpods pro 2', np.uint32(11))]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_proba(X=\"iPhone 13 Pro Max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returned a number of possible matches, with a score for each match. The score is the cosine similarity between the query and the product description. We could set the limit to say the top 2 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('iphone 13 pro max', np.uint32(0)), ('iphone 13 promax', np.uint32(1))]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.limit = 2\n",
    "predictor.predict_proba(X=\"iPhone 13 Pro Max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Classification Example\n",
    "\n",
    "Let's assume you're trying to label a batch of survey responses, instead of searching the existence of some query. For example, let's say we are trying to standardize the responses using the training data provided. You could use the BOWPredictor to predict the label for each survey response.\n",
    "\n",
    "This resembles a supervised learning problem where we have a set of features (X) and a set of labels (y). We can fit the BOWPredictor on the training data and then use it to predict the label for each survey response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone 13 pro max',\n",
       " 'iphone 13 promax',\n",
       " 'iphone 13 pro',\n",
       " 'iphone 13',\n",
       " 'samsung galaxy s21',\n",
       " 'samsung galaxy s21 ultra',\n",
       " 'samsung s21',\n",
       " 'samsung s21 ultra',\n",
       " 'macbook pro m1',\n",
       " 'macbook pro m1 max',\n",
       " 'macbook m1',\n",
       " 'macbook m1 pro',\n",
       " 'airpods pro 2nd gen',\n",
       " 'airpods pro 2',\n",
       " 'airpods pro second generation',\n",
       " 'airpods 2nd gen',\n",
       " 'airpods 3rd gen',\n",
       " 'airpods 3',\n",
       " 'galaxy buds pro',\n",
       " 'samsung buds pro',\n",
       " 'galaxy buds 2',\n",
       " 'samsung buds 2',\n",
       " 'tide original detergent',\n",
       " 'tide detergent original',\n",
       " 'tide pods original',\n",
       " 'tide pods spring meadow',\n",
       " 'tide spring meadow detergent',\n",
       " 'bounty paper towels',\n",
       " 'bounty select a size',\n",
       " 'bounty select size',\n",
       " 'charmin ultra soft',\n",
       " 'charmin toilet paper ultra soft',\n",
       " 'charmin ultra strong',\n",
       " 'clorox original',\n",
       " 'clorox bleach',\n",
       " 'clorox disinfecting wipes',\n",
       " 'lysol disinfectant spray',\n",
       " 'lysol spray',\n",
       " 'lysol wipes',\n",
       " 'dawn dish soap original',\n",
       " 'dawn original',\n",
       " 'dawn ultra dish soap',\n",
       " 'cheerios original',\n",
       " 'plain cheerios',\n",
       " 'honey nut cheerios',\n",
       " 'frosted flakes',\n",
       " 'kelloggs frosted flakes',\n",
       " 'fruit loops',\n",
       " 'froot loops',\n",
       " 'kelloggs fruit loops',\n",
       " 'coca cola original',\n",
       " 'coke classic',\n",
       " 'diet coke',\n",
       " 'coke zero',\n",
       " 'coca cola zero',\n",
       " 'pepsi original',\n",
       " 'diet pepsi',\n",
       " 'pepsi zero sugar',\n",
       " 'doritos nacho cheese',\n",
       " 'doritos cool ranch',\n",
       " 'lays classic',\n",
       " 'lays original',\n",
       " 'lays bbq',\n",
       " 'cheetos original',\n",
       " 'cheetos crunchy',\n",
       " 'cheetos flamin hot',\n",
       " 'oreo original',\n",
       " 'oreo double stuf',\n",
       " 'oreo mint creme',\n",
       " 'chips ahoy original',\n",
       " 'chips ahoy chewy',\n",
       " 'nutella hazelnut spread',\n",
       " 'jif creamy peanut butter',\n",
       " 'jif crunchy',\n",
       " 'skippy creamy',\n",
       " 'skippy natural pb',\n",
       " 'kraft mac and cheese original',\n",
       " 'kraft mac n cheese',\n",
       " 'kraft dinner original',\n",
       " 'campbell chicken noodle soup',\n",
       " 'campbells tomato soup',\n",
       " 'progresso chicken noodle',\n",
       " 'heinz ketchup',\n",
       " 'heinz tomato ketchup',\n",
       " \"french's yellow mustard\",\n",
       " 'frenchs mustard',\n",
       " 'sriracha hot sauce',\n",
       " 'sriracha sauce',\n",
       " 'tabasco original',\n",
       " 'tabasco sauce',\n",
       " 'ben and jerrys chunky monkey',\n",
       " 'ben & jerrys phish food',\n",
       " 'haagen dazs vanilla',\n",
       " 'haagen dazs chocolate',\n",
       " 'breyers natural vanilla',\n",
       " 'quilted northern ultra plush',\n",
       " 'quilted northern toilet paper',\n",
       " 'angel soft toilet paper',\n",
       " 'angel soft bath tissue',\n",
       " 'scott toilet paper',\n",
       " 'brawny paper towels',\n",
       " 'sparkle paper towels',\n",
       " 'viva paper towels',\n",
       " 'cascade complete pods',\n",
       " 'cascade dishwasher pods',\n",
       " 'finish quantum pods',\n",
       " 'finish dishwasher tabs',\n",
       " 'swiffer wet jet',\n",
       " 'swiffer wetjet refill',\n",
       " 'swiffer duster',\n",
       " 'febreze air fresh linen',\n",
       " 'febreze air effects',\n",
       " 'glade clean linen spray',\n",
       " 'glade plugins refill',\n",
       " 'air wick essential oils refill',\n",
       " 'air wick plug in',\n",
       " 'duracell aa batteries',\n",
       " 'duracell aaa',\n",
       " 'energizer aa',\n",
       " 'energizer aaa batteries',\n",
       " 'glad tall kitchen bags',\n",
       " 'glad trash bags',\n",
       " 'hefty ultra strong',\n",
       " 'ziploc freezer gallon',\n",
       " 'ziploc storage quart',\n",
       " 'reynolds wrap aluminum foil',\n",
       " 'reynolds heavy duty foil',\n",
       " 'glad press n seal',\n",
       " 'saran premium wrap',\n",
       " 'dixie paper plates',\n",
       " 'solo red cups',\n",
       " 'red solo cups',\n",
       " 'bounty napkins',\n",
       " 'vanity fair napkins']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_transform(X=df['X'].tolist(), y=df['y_true'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.str_('iPhone 13 Pro Max'), np.str_('iPhone 13 Pro'), np.str_('iPhone 13')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that 'iphone 12' is not in the labeled data, so we cannot\n",
    "# predict it.\n",
    "predictor.predict(X=['iphone 13 pro max', 'iphone 13 pro', 'iphone 12'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Clustering Example\n",
    "\n",
    "Let's assume you're trying to cluster a batch of survey responses. You could use the predictor to cluster the survey responses.\n",
    "\n",
    "This resembles an unsupervised learning problem where we have a set of features (X) and we want to cluster the data into different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone 13 pro max',\n",
       " 'iphone 13 promax',\n",
       " 'iphone 13 pro',\n",
       " 'iphone 13',\n",
       " 'samsung galaxy s21',\n",
       " 'samsung galaxy s21 ultra',\n",
       " 'samsung s21',\n",
       " 'samsung s21 ultra',\n",
       " 'macbook pro m1',\n",
       " 'macbook pro m1 max',\n",
       " 'macbook m1',\n",
       " 'macbook m1 pro',\n",
       " 'airpods pro 2nd gen',\n",
       " 'airpods pro 2',\n",
       " 'airpods pro second generation',\n",
       " 'airpods 2nd gen',\n",
       " 'airpods 3rd gen',\n",
       " 'airpods 3',\n",
       " 'galaxy buds pro',\n",
       " 'samsung buds pro',\n",
       " 'galaxy buds 2',\n",
       " 'samsung buds 2',\n",
       " 'tide original detergent',\n",
       " 'tide detergent original',\n",
       " 'tide pods original',\n",
       " 'tide pods spring meadow',\n",
       " 'tide spring meadow detergent',\n",
       " 'bounty paper towels',\n",
       " 'bounty select a size',\n",
       " 'bounty select size',\n",
       " 'charmin ultra soft',\n",
       " 'charmin toilet paper ultra soft',\n",
       " 'charmin ultra strong',\n",
       " 'clorox original',\n",
       " 'clorox bleach',\n",
       " 'clorox disinfecting wipes',\n",
       " 'lysol disinfectant spray',\n",
       " 'lysol spray',\n",
       " 'lysol wipes',\n",
       " 'dawn dish soap original',\n",
       " 'dawn original',\n",
       " 'dawn ultra dish soap',\n",
       " 'cheerios original',\n",
       " 'plain cheerios',\n",
       " 'honey nut cheerios',\n",
       " 'frosted flakes',\n",
       " 'kelloggs frosted flakes',\n",
       " 'fruit loops',\n",
       " 'froot loops',\n",
       " 'kelloggs fruit loops',\n",
       " 'coca cola original',\n",
       " 'coke classic',\n",
       " 'diet coke',\n",
       " 'coke zero',\n",
       " 'coca cola zero',\n",
       " 'pepsi original',\n",
       " 'diet pepsi',\n",
       " 'pepsi zero sugar',\n",
       " 'doritos nacho cheese',\n",
       " 'doritos cool ranch',\n",
       " 'lays classic',\n",
       " 'lays original',\n",
       " 'lays bbq',\n",
       " 'cheetos original',\n",
       " 'cheetos crunchy',\n",
       " 'cheetos flamin hot',\n",
       " 'oreo original',\n",
       " 'oreo double stuf',\n",
       " 'oreo mint creme',\n",
       " 'chips ahoy original',\n",
       " 'chips ahoy chewy',\n",
       " 'nutella hazelnut spread',\n",
       " 'jif creamy peanut butter',\n",
       " 'jif crunchy',\n",
       " 'skippy creamy',\n",
       " 'skippy natural pb',\n",
       " 'kraft mac and cheese original',\n",
       " 'kraft mac n cheese',\n",
       " 'kraft dinner original',\n",
       " 'campbell chicken noodle soup',\n",
       " 'campbells tomato soup',\n",
       " 'progresso chicken noodle',\n",
       " 'heinz ketchup',\n",
       " 'heinz tomato ketchup',\n",
       " \"french's yellow mustard\",\n",
       " 'frenchs mustard',\n",
       " 'sriracha hot sauce',\n",
       " 'sriracha sauce',\n",
       " 'tabasco original',\n",
       " 'tabasco sauce',\n",
       " 'ben and jerrys chunky monkey',\n",
       " 'ben & jerrys phish food',\n",
       " 'haagen dazs vanilla',\n",
       " 'haagen dazs chocolate',\n",
       " 'breyers natural vanilla',\n",
       " 'quilted northern ultra plush',\n",
       " 'quilted northern toilet paper',\n",
       " 'angel soft toilet paper',\n",
       " 'angel soft bath tissue',\n",
       " 'scott toilet paper',\n",
       " 'brawny paper towels',\n",
       " 'sparkle paper towels',\n",
       " 'viva paper towels',\n",
       " 'cascade complete pods',\n",
       " 'cascade dishwasher pods',\n",
       " 'finish quantum pods',\n",
       " 'finish dishwasher tabs',\n",
       " 'swiffer wet jet',\n",
       " 'swiffer wetjet refill',\n",
       " 'swiffer duster',\n",
       " 'febreze air fresh linen',\n",
       " 'febreze air effects',\n",
       " 'glade clean linen spray',\n",
       " 'glade plugins refill',\n",
       " 'air wick essential oils refill',\n",
       " 'air wick plug in',\n",
       " 'duracell aa batteries',\n",
       " 'duracell aaa',\n",
       " 'energizer aa',\n",
       " 'energizer aaa batteries',\n",
       " 'glad tall kitchen bags',\n",
       " 'glad trash bags',\n",
       " 'hefty ultra strong',\n",
       " 'ziploc freezer gallon',\n",
       " 'ziploc storage quart',\n",
       " 'reynolds wrap aluminum foil',\n",
       " 'reynolds heavy duty foil',\n",
       " 'glad press n seal',\n",
       " 'saran premium wrap',\n",
       " 'dixie paper plates',\n",
       " 'solo red cups',\n",
       " 'red solo cups',\n",
       " 'bounty napkins',\n",
       " 'vanity fair napkins']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_transform(X=df['X'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first survey response and see what cluster it belongs to by seeing which values have the highest probability, which will always include itself. A score cutoff could be applied to simluate cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('iphone 13 pro max', np.uint32(0)), ('iphone 13 promax', np.uint32(1))]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_proba(X=df['X'].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate Performance\n",
    "\n",
    "Let's validate the performance of the predictor as a classifier (supervised learning). We can create several version of the predictor with different parameters and validate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Grid Search\n",
    "\n",
    "Since the predictor is not a true learner, it doesn't make sense to use a grid search with cross validation. Instead, we can manually create a list of predictors with different parameters and validate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "all_predictors = []\n",
    "\n",
    "for algorithm in ALGORITHMS.keys():\n",
    "    for method in METHODS:\n",
    "        all_predictors.append(FuzzyPredictor(\n",
    "            algorithm=algorithm, method=method\n",
    "        ))\n",
    "\n",
    "print(len(all_predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Method</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jaro</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jaro_winkler</td>\n",
       "      <td>normalized_similarity</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jaro_winkler</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jaro_winkler</td>\n",
       "      <td>normalized_distance</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jaro_winkler</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jaro</td>\n",
       "      <td>normalized_similarity</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jaro</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jaro</td>\n",
       "      <td>normalized_distance</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>indel</td>\n",
       "      <td>normalized_distance</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.8769</td>\n",
       "      <td>0.8806</td>\n",
       "      <td>0.8806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>indel</td>\n",
       "      <td>normalized_similarity</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>0.8731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>indel</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.8456</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>0.8657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>levenshtein</td>\n",
       "      <td>normalized_similarity</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>0.8693</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.8507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>damerau_levenshtein</td>\n",
       "      <td>normalized_similarity</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>0.8693</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.8507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>levenshtein</td>\n",
       "      <td>normalized_distance</td>\n",
       "      <td>0.8278</td>\n",
       "      <td>0.8582</td>\n",
       "      <td>0.8433</td>\n",
       "      <td>0.8433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>damerau_levenshtein</td>\n",
       "      <td>normalized_distance</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.8581</td>\n",
       "      <td>0.8433</td>\n",
       "      <td>0.8433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>levenshtein</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.8115</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8358</td>\n",
       "      <td>0.8358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>damerau_levenshtein</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.8234</td>\n",
       "      <td>0.8284</td>\n",
       "      <td>0.8284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>indel</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.7729</td>\n",
       "      <td>0.8058</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.7910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hamming</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.7165</td>\n",
       "      <td>0.7460</td>\n",
       "      <td>0.7537</td>\n",
       "      <td>0.7537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hamming</td>\n",
       "      <td>normalized_distance</td>\n",
       "      <td>0.6741</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.7313</td>\n",
       "      <td>0.7313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hamming</td>\n",
       "      <td>normalized_similarity</td>\n",
       "      <td>0.6671</td>\n",
       "      <td>0.6730</td>\n",
       "      <td>0.7239</td>\n",
       "      <td>0.7239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>levenshtein</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.6176</td>\n",
       "      <td>0.6628</td>\n",
       "      <td>0.6493</td>\n",
       "      <td>0.6493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damerau_levenshtein</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.6059</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.6418</td>\n",
       "      <td>0.6418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hamming</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.4465</td>\n",
       "      <td>0.4952</td>\n",
       "      <td>0.4925</td>\n",
       "      <td>0.4925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Algorithm                 Method  F1 Score  Precision  Recall  \\\n",
       "12                 jaro               distance    0.8965     0.9316  0.9030   \n",
       "19         jaro_winkler  normalized_similarity    0.8965     0.9316  0.9030   \n",
       "18         jaro_winkler             similarity    0.8965     0.9316  0.9030   \n",
       "17         jaro_winkler    normalized_distance    0.8965     0.9316  0.9030   \n",
       "16         jaro_winkler               distance    0.8965     0.9316  0.9030   \n",
       "15                 jaro  normalized_similarity    0.8965     0.9316  0.9030   \n",
       "14                 jaro             similarity    0.8965     0.9316  0.9030   \n",
       "13                 jaro    normalized_distance    0.8965     0.9316  0.9030   \n",
       "9                 indel    normalized_distance    0.8619     0.8769  0.8806   \n",
       "11                indel  normalized_similarity    0.8520     0.8657  0.8731   \n",
       "10                indel             similarity    0.8456     0.8588  0.8657   \n",
       "23          levenshtein  normalized_similarity    0.8376     0.8693  0.8507   \n",
       "3   damerau_levenshtein  normalized_similarity    0.8376     0.8693  0.8507   \n",
       "21          levenshtein    normalized_distance    0.8278     0.8582  0.8433   \n",
       "1   damerau_levenshtein    normalized_distance    0.8276     0.8581  0.8433   \n",
       "22          levenshtein             similarity    0.8115     0.8346  0.8358   \n",
       "2   damerau_levenshtein             similarity    0.8015     0.8234  0.8284   \n",
       "8                 indel               distance    0.7729     0.8058  0.7910   \n",
       "6               hamming             similarity    0.7165     0.7460  0.7537   \n",
       "5               hamming    normalized_distance    0.6741     0.6786  0.7313   \n",
       "7               hamming  normalized_similarity    0.6671     0.6730  0.7239   \n",
       "20          levenshtein               distance    0.6176     0.6628  0.6493   \n",
       "0   damerau_levenshtein               distance    0.6059     0.6529  0.6418   \n",
       "4               hamming               distance    0.4465     0.4952  0.4925   \n",
       "\n",
       "    Accuracy  \n",
       "12    0.9030  \n",
       "19    0.9030  \n",
       "18    0.9030  \n",
       "17    0.9030  \n",
       "16    0.9030  \n",
       "15    0.9030  \n",
       "14    0.9030  \n",
       "13    0.9030  \n",
       "9     0.8806  \n",
       "11    0.8731  \n",
       "10    0.8657  \n",
       "23    0.8507  \n",
       "3     0.8507  \n",
       "21    0.8433  \n",
       "1     0.8433  \n",
       "22    0.8358  \n",
       "2     0.8284  \n",
       "8     0.7910  \n",
       "6     0.7537  \n",
       "5     0.7313  \n",
       "7     0.7239  \n",
       "20    0.6493  \n",
       "0     0.6418  \n",
       "4     0.4925  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty list to store results.\n",
    "results = []\n",
    "\n",
    "for predictor in all_predictors:\n",
    "\n",
    "    # Fit the predictor on the product descriptions.\n",
    "    predictor.fit_transform(df['y_true'].unique().tolist())\n",
    "    \n",
    "    # Predict the product descriptions.\n",
    "    predictions = predictor.predict(X=df['X'].tolist())\n",
    "    \n",
    "    # Store results in a dictionary.\n",
    "    result = {\n",
    "        'Algorithm': predictor.algorithm,\n",
    "        'Method': predictor.method,\n",
    "        'F1 Score': f1_score(df['y_true'], predictions, average='weighted'),\n",
    "        'Precision': precision_score(\n",
    "            df['y_true'], predictions, average='weighted', zero_division=0\n",
    "        ),\n",
    "        'Recall': recall_score(df['y_true'], predictions, average='weighted'),\n",
    "        'Accuracy': accuracy_score(df['y_true'], predictions)\n",
    "    }\n",
    "    \n",
    "    # Append to results list.\n",
    "    results.append(result)\n",
    "\n",
    "# Convert to DataFrame.\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round numeric columns to 4 decimal places.\n",
    "numeric_columns = ['F1 Score', 'Precision', 'Recall', 'Accuracy']\n",
    "results_df[numeric_columns] = results_df[numeric_columns].round(4)\n",
    "\n",
    "# Sort the results by F1 Score in descending order.\n",
    "results_df.sort_values(by='F1 Score', ascending=False, inplace=True)\n",
    "\n",
    "# Display the results.\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Validation with Grid Search\n",
    "\n",
    "However, the predictor can theoretically be plugged into an existing grid search pipeline without causing errors. It follows the sklearn estimator API, so it can be used in a grid search pipeline.\n",
    "\n",
    "One work-around is to define the CV such that it uses the same folds for each parameter combination. To do that, we pass a CV value where the indexes are the same for both test and train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    FuzzyPredictor(),\n",
    "    {\n",
    "        'algorithm': list(ALGORITHMS.keys()),\n",
    "        'method': METHODS\n",
    "    },\n",
    "    cv=[(np.arange(len(df)), np.arange(len(df)))],\n",
    "    scoring='f1_weighted'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'algorithm': 'jaro', 'method': 'distance'}\n",
      "Best score: 0.8865671641791045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'algorithm': 'jaro', 'method': 'normalized_si...</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'algorithm': 'jaro', 'method': 'distance'}</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'algorithm': 'jaro', 'method': 'normalized_di...</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'algorithm': 'jaro', 'method': 'similarity'}</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'algorithm': 'jaro_winkler', 'method': 'norma...</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'algorithm': 'jaro_winkler', 'method': 'simil...</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'algorithm': 'jaro_winkler', 'method': 'norma...</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'algorithm': 'jaro_winkler', 'method': 'dista...</td>\n",
       "      <td>0.886567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'algorithm': 'indel', 'method': 'similarity'}</td>\n",
       "      <td>0.879104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'algorithm': 'indel', 'method': 'normalized_d...</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'algorithm': 'levenshtein', 'method': 'simila...</td>\n",
       "      <td>0.855224</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'algorithm': 'damerau_levenshtein', 'method':...</td>\n",
       "      <td>0.853980</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'algorithm': 'indel', 'method': 'normalized_s...</td>\n",
       "      <td>0.851990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'algorithm': 'damerau_levenshtein', 'method':...</td>\n",
       "      <td>0.837704</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'algorithm': 'levenshtein', 'method': 'normal...</td>\n",
       "      <td>0.837562</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'algorithm': 'levenshtein', 'method': 'normal...</td>\n",
       "      <td>0.827612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'algorithm': 'damerau_levenshtein', 'method':...</td>\n",
       "      <td>0.827612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'algorithm': 'indel', 'method': 'distance'}</td>\n",
       "      <td>0.783085</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'algorithm': 'hamming', 'method': 'similarity'}</td>\n",
       "      <td>0.728269</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'algorithm': 'hamming', 'method': 'normalized...</td>\n",
       "      <td>0.680792</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'algorithm': 'hamming', 'method': 'normalized...</td>\n",
       "      <td>0.677061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'algorithm': 'levenshtein', 'method': 'distan...</td>\n",
       "      <td>0.616145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'algorithm': 'damerau_levenshtein', 'method':...</td>\n",
       "      <td>0.614736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'algorithm': 'hamming', 'method': 'distance'}</td>\n",
       "      <td>0.433151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_score  \\\n",
       "15  {'algorithm': 'jaro', 'method': 'normalized_si...         0.886567   \n",
       "12        {'algorithm': 'jaro', 'method': 'distance'}         0.886567   \n",
       "13  {'algorithm': 'jaro', 'method': 'normalized_di...         0.886567   \n",
       "14      {'algorithm': 'jaro', 'method': 'similarity'}         0.886567   \n",
       "19  {'algorithm': 'jaro_winkler', 'method': 'norma...         0.886567   \n",
       "18  {'algorithm': 'jaro_winkler', 'method': 'simil...         0.886567   \n",
       "17  {'algorithm': 'jaro_winkler', 'method': 'norma...         0.886567   \n",
       "16  {'algorithm': 'jaro_winkler', 'method': 'dista...         0.886567   \n",
       "10     {'algorithm': 'indel', 'method': 'similarity'}         0.879104   \n",
       "9   {'algorithm': 'indel', 'method': 'normalized_d...         0.861940   \n",
       "22  {'algorithm': 'levenshtein', 'method': 'simila...         0.855224   \n",
       "2   {'algorithm': 'damerau_levenshtein', 'method':...         0.853980   \n",
       "11  {'algorithm': 'indel', 'method': 'normalized_s...         0.851990   \n",
       "1   {'algorithm': 'damerau_levenshtein', 'method':...         0.837704   \n",
       "21  {'algorithm': 'levenshtein', 'method': 'normal...         0.837562   \n",
       "23  {'algorithm': 'levenshtein', 'method': 'normal...         0.827612   \n",
       "3   {'algorithm': 'damerau_levenshtein', 'method':...         0.827612   \n",
       "8        {'algorithm': 'indel', 'method': 'distance'}         0.783085   \n",
       "6    {'algorithm': 'hamming', 'method': 'similarity'}         0.728269   \n",
       "7   {'algorithm': 'hamming', 'method': 'normalized...         0.680792   \n",
       "5   {'algorithm': 'hamming', 'method': 'normalized...         0.677061   \n",
       "20  {'algorithm': 'levenshtein', 'method': 'distan...         0.616145   \n",
       "0   {'algorithm': 'damerau_levenshtein', 'method':...         0.614736   \n",
       "4      {'algorithm': 'hamming', 'method': 'distance'}         0.433151   \n",
       "\n",
       "    std_test_score  \n",
       "15             0.0  \n",
       "12             0.0  \n",
       "13             0.0  \n",
       "14             0.0  \n",
       "19             0.0  \n",
       "18             0.0  \n",
       "17             0.0  \n",
       "16             0.0  \n",
       "10             0.0  \n",
       "9              0.0  \n",
       "22             0.0  \n",
       "2              0.0  \n",
       "11             0.0  \n",
       "1              0.0  \n",
       "21             0.0  \n",
       "23             0.0  \n",
       "3              0.0  \n",
       "8              0.0  \n",
       "6              0.0  \n",
       "7              0.0  \n",
       "5              0.0  \n",
       "20             0.0  \n",
       "0              0.0  \n",
       "4              0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the grid search.\n",
    "grid_search.fit(df['X'].tolist(), df['y_true'].tolist())\n",
    "\n",
    "# Print results.\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Get detailed results in a DataFrame.\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results = results.sort_values('rank_test_score')\n",
    "display(results[['params', 'mean_test_score', 'std_test_score']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-search-0Fhw7W71-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
